{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eac13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install llama-index-llms-huggingface-api llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2866b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a82351cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naswahmanandhar/Desktop/Multi Agent/ma_env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_token= os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    "    token=hf_token,\n",
    "    provider=\"auto\"\n",
    ")\n",
    "\n",
    "response= llm.complete(\"Hello, how are you?\")\n",
    "print (response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813c261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f52560",
   "metadata": {},
   "source": [
    "extracting persona of people attendng alfred's paty, storing locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e0e56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "dataset = load_dataset(path=\"dvilasuero/finepersonas-v0.1-tiny\", split=\"train\")\n",
    "\n",
    "Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "for i, persona in enumerate(dataset):\n",
    "    with open(Path(\"data\") / f\"persona_{i}.txt\", \"w\") as f:\n",
    "        f.write(persona[\"persona\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c0e9d9",
   "metadata": {},
   "source": [
    "Loading the personas using SImpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f8d0c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=\"data\")\n",
    "documents= reader.load_data()\n",
    "len (documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f9bcf0",
   "metadata": {},
   "source": [
    "After loading, break them to pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e257e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a39e8715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 15:00:56,903 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-11-20 15:00:57,004 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "2025-11-20 15:01:02,264 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes ingested: 5000\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "db= chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(name=\"alfred\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_overlap=0),\n",
    "        HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"),\n",
    "    ],\n",
    "    vector_store=vector_store  \n",
    ")\n",
    "\n",
    "nodes = await pipeline.arun(documents=documents)  \n",
    "print(\"Number of nodes ingested:\", len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a967cc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 15:01:14,466 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "2025-11-20 15:01:19,904 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dde8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install llama-index huggingface-hub transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ab73d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='<think>\\nOkay, let\\'s see. The user wants me to respond using a persona that describes an author and their travel experiences. The context mentions a travel blogger focused on cultural exploration and language, specifically Eastern European history and a background in linguistics or education.\\n\\nFirst, I need to create a persona that fits these elements. The persona should sound like someone who\\'s not just a traveler but also someone who deeply understands the cultural and historical aspects of Eastern Europe. They probably have a rich background in languages and education, so their writing might be detailed and informative.\\n\\nI should start with a name that sounds authentic, maybe a blend of Eastern European and modern elements. Let\\'s go with \"Ivan Petrovich\" – sounds Russian, but not too traditional. Then a title, maybe something like \"Cultural Navigator\" to highlight their expertise.\\n\\nNext, the travel experiences should reflect their interests. They might visit historical sites, engage with local communities, and share insights on the languages and their significance. Mentioning specific regions like Poland, Ukraine, and Belarus makes sense because those are Eastern European countries with rich history and diverse cultures.\\n\\nIncluding anecdotes would add a personal touch. For example, ordering a traditional meal in a local café and learning a few phrases, or participating in a local', source_nodes=[NodeWithScore(node=TextNode(id_='1c93aef8-e132-48b1-8d25-3ed574bbe7bd', embedding=None, metadata={'file_path': '/Users/naswahmanandhar/Desktop/Multi Agent/data/persona_783.txt', 'file_name': 'persona_783.txt', 'file_type': 'text/plain', 'file_size': 160, 'creation_date': '2025-11-20', 'last_modified_date': '2025-11-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8b86db73-4e6b-4ba2-8bef-fa1d54a54ddf', node_type='4', metadata={'file_path': '/Users/naswahmanandhar/Desktop/Multi Agent/data/persona_783.txt', 'file_name': 'persona_783.txt', 'file_type': 'text/plain', 'file_size': 160, 'creation_date': '2025-11-20', 'last_modified_date': '2025-11-20'}, hash='da578dfbb2d7a548c4a0010c877412b90aeb87424b58fdd8f3402bdd2b00b6c3')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='A travel blogger focused on cultural exploration and language, likely with an interest in Eastern European history and a background in linguistics or education.', mimetype='text/plain', start_char_idx=0, end_char_idx=160, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6564720685907166)], metadata={'1c93aef8-e132-48b1-8d25-3ed574bbe7bd': {'file_path': '/Users/naswahmanandhar/Desktop/Multi Agent/data/persona_783.txt', 'file_name': 'persona_783.txt', 'file_type': 'text/plain', 'file_size': 160, 'creation_date': '2025-11-20', 'last_modified_date': '2025-11-20'}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()  # This is needed to run the query engine\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"HuggingFaceTB/SmolLM3-3B\",\n",
    "    token=os.getenv(\"HF_TOKEN\"),\n",
    "    temperature=0.7,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")\n",
    "response = query_engine.query(\n",
    "    \"Respond using a persona that describes author and travel experiences?\"\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a1fcc5",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "068564bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "evaluator= FaithfulnessEvaluator(llm=llm)\n",
    "eval_result= evaluator.evaluate_response(response=response)\n",
    "eval_result.passing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430d72a0",
   "metadata": {},
   "source": [
    "Functional tools in LLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7ed7858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting weather for Kathmandu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ToolOutput(blocks=[TextBlock(block_type='text', text='The weather in Kathmandu is sunny')], tool_name='weather_tool', raw_input={'args': ('Kathmandu',), 'kwargs': {}}, raw_output='The weather in Kathmandu is sunny', is_error=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def get_weather(location:str)->str:\n",
    "    print(f\"Getting weather for {location}\")\n",
    "    return f\"The weather in {location} is sunny\"\n",
    "\n",
    "tool= FunctionTool.from_defaults(\n",
    "    get_weather,\n",
    "    name=\"weather_tool\",\n",
    "    description=\"It is used to find the descrption of a location\"\n",
    ")\n",
    "tool.call(\"Kathmandu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9879c2d",
   "metadata": {},
   "source": [
    "Creating a QueryEngineTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79297aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 15:36:59,355 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "2025-11-20 15:37:03,873 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ToolOutput(blocks=[TextBlock(block_type='text', text=\"<think>\\nOkay, let's see. The user is asking about research on the impact of AI on the future of work and society, and they provided a context about a machine learning researcher focused on natural language processing and AI prompts. Hmm, the context mentions developing AI prompts to enhance performance, decision-making, and ethics. But how does that relate to the impact on work and society?\\n\\nWait, the user wants to know if the researcher's work is related to that topic. The context doesn't explicitly mention work or society. The researcher is working on AI prompts for enhancing AI's performance, decisions, and ethics. Maybe the researcher's work could indirectly affect work and society through better AI systems. For example, if AI is more efficient in decision-making, it might lead to more job opportunities or changes in the workforce. But the context doesn't specify that. The answer needs to be based strictly on the given information. Since the context doesn't mention work or society, the answer should probably not assume that. The researcher's focus is on AI prompts, not the broader societal impacts. So the answer would be that the context doesn't directly address the impact on work and society, but the researcher's work could potentially influence those areas through AI enhancements. But the user wants a direct\")], tool_name='name', raw_input={'input': 'Responds about research on the impact of AI on the future of work and society?'}, raw_output=Response(response=\"<think>\\nOkay, let's see. The user is asking about research on the impact of AI on the future of work and society, and they provided a context about a machine learning researcher focused on natural language processing and AI prompts. Hmm, the context mentions developing AI prompts to enhance performance, decision-making, and ethics. But how does that relate to the impact on work and society?\\n\\nWait, the user wants to know if the researcher's work is related to that topic. The context doesn't explicitly mention work or society. The researcher is working on AI prompts for enhancing AI's performance, decisions, and ethics. Maybe the researcher's work could indirectly affect work and society through better AI systems. For example, if AI is more efficient in decision-making, it might lead to more job opportunities or changes in the workforce. But the context doesn't specify that. The answer needs to be based strictly on the given information. Since the context doesn't mention work or society, the answer should probably not assume that. The researcher's focus is on AI prompts, not the broader societal impacts. So the answer would be that the context doesn't directly address the impact on work and society, but the researcher's work could potentially influence those areas through AI enhancements. But the user wants a direct\", source_nodes=[NodeWithScore(node=TextNode(id_='98843835-52d2-46d6-bf69-1cc0fbfadf73', embedding=None, metadata={'file_path': '/Users/naswahmanandhar/Desktop/Multi Agent/data/persona_1587.txt', 'file_name': 'persona_1587.txt', 'file_type': 'text/plain', 'file_size': 215, 'creation_date': '2025-11-20', 'last_modified_date': '2025-11-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b83e3501-00f0-4ef0-9100-68436d4fdf69', node_type='4', metadata={'file_path': '/Users/naswahmanandhar/Desktop/Multi Agent/data/persona_1587.txt', 'file_name': 'persona_1587.txt', 'file_type': 'text/plain', 'file_size': 215, 'creation_date': '2025-11-20', 'last_modified_date': '2025-11-20'}, hash='ddbe2879627f037efe27c318f05c5df9dd77062cfb62e49856f2e255f8bf2c25')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='A machine learning researcher focused on natural language processing and artificial intelligence, with a strong emphasis on developing and utilizing AI prompts to enhance AI performance, decision-making, and ethics.', mimetype='text/plain', start_char_idx=0, end_char_idx=215, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.4963013313432915), NodeWithScore(node=TextNode(id_='c87b0ff9-56fc-4358-b9eb-eb54230eba60', embedding=None, metadata={'file_path': '/Users/naswahmanandhar/Desktop/Multi Agent/data/persona_1587.txt', 'file_name': 'persona_1587.txt', 'file_type': 'text/plain', 'file_size': 215, 'creation_date': '2025-11-20', 'last_modified_date': '2025-11-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='abaa60af-a024-461e-962c-d73f24654cbd', node_type='4', metadata={'file_path': '/Users/naswahmanandhar/Desktop/Multi Agent/data/persona_1587.txt', 'file_name': 'persona_1587.txt', 'file_type': 'text/plain', 'file_size': 215, 'creation_date': '2025-11-20', 'last_modified_date': '2025-11-20'}, hash='ddbe2879627f037efe27c318f05c5df9dd77062cfb62e49856f2e255f8bf2c25')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='A machine learning researcher focused on natural language processing and artificial intelligence, with a strong emphasis on developing and utilizing AI prompts to enhance AI performance, decision-making, and ethics.', mimetype='text/plain', start_char_idx=0, end_char_idx=215, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.4963013313432915)], metadata={'98843835-52d2-46d6-bf69-1cc0fbfadf73': {'file_path': '/Users/naswahmanandhar/Desktop/Multi Agent/data/persona_1587.txt', 'file_name': 'persona_1587.txt', 'file_type': 'text/plain', 'file_size': 215, 'creation_date': '2025-11-20', 'last_modified_date': '2025-11-20'}, 'c87b0ff9-56fc-4358-b9eb-eb54230eba60': {'file_path': '/Users/naswahmanandhar/Desktop/Multi Agent/data/persona_1587.txt', 'file_name': 'persona_1587.txt', 'file_type': 'text/plain', 'file_size': 215, 'creation_date': '2025-11-20', 'last_modified_date': '2025-11-20'}}), is_error=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "db= chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(name=\"alfred\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "embed_model= HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"HuggingFaceTB/SmolLM3-3B\",\n",
    "    token=os.getenv(\"HF_TOKEN\"),\n",
    "    temperature=0.7,\n",
    "    max_tokens=200\n",
    ")\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, embed_model=embed_model\n",
    ")\n",
    "\n",
    "query_engine= index.as_query_engine(llm=llm)\n",
    "\n",
    "tool= QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    name=\"name\",\n",
    "    description=\"some desc\"\n",
    ")\n",
    "\n",
    "await tool.acall(\"Responds about research on the impact of AI on the future of work and society?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5359dda7",
   "metadata": {},
   "source": [
    "gents in LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "281df58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "# define sample Tool -- type annotations, function names, and docstrings, are all included in parsed schemas!\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies two integers and returns the resulting integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "# initialize llm\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"HuggingFaceTB/SmolLM3-3B\",\n",
    "    token=os.getenv(\"HF_TOKEN\"),\n",
    "    temperature=0.7,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "# initialize agent\n",
    "agent = AgentWorkflow.from_tools_or_functions(\n",
    "    [FunctionTool.from_defaults(multiply)],\n",
    "    llm=llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d40546",
   "metadata": {},
   "source": [
    "RAG with agents\n",
    "Provide promt+query+relevant data to llm, llm provides reponse to user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3513e666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "query_engine = index.as_query_engine(llm=llm, similarity_top_k=3) # as shown in the Components in LlamaIndex section\n",
    "\n",
    "query_engine_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    name=\"name\",\n",
    "    description=\"a specific description\",\n",
    "    return_direct=False,\n",
    ")\n",
    "query_engine_agent = AgentWorkflow.from_tools_or_functions(\n",
    "    [query_engine_tool],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that has access to a database containing persona descriptions. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb627cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The user is asking to add 5 and 3. I need to figure out which tool to use here. The available tools are add, subtract, and handoff. Since the task is to add two numbers, the add tool should be the right choice.\n",
      "\n",
      "First, I'll start with a thought to note that I need to use a tool. Then, I'll use the add tool. The parameters for the add tool are a and b, both integers. The numbers here are 5 and 3. So the action input should be a JSON object with a: 5 and b: 3. \n",
      "\n",
      "Wait, the action input format needs to be in the correct JSON structure. The user mentioned that the Action Input should be in JSON format representing the kwargs. So for the add tool, the input would be {\"a\": 5, \"b\": 3}. \n",
      "\n",
      "After sending the action input, the tool should respond with the result. Let me check if there's any reason to handoff here. Since the task is straightforward addition, there's no need to handoff to another agent. \n",
      "\n",
      "So the response should be the sum of 5 and 3, which is 8. Then, I\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import (\n",
    "    AgentWorkflow,\n",
    "    FunctionAgent,\n",
    "    ReActAgent,\n",
    ")\n",
    "\n",
    "# Define some tools\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract two numbers.\"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "# Create agent configs\n",
    "# NOTE: we can use FunctionAgent or ReActAgent here.\n",
    "# FunctionAgent works for LLMs with a function calling API.\n",
    "# ReActAgent works for any LLM.\n",
    "calculator_agent = ReActAgent(\n",
    "    name=\"calculator\",\n",
    "    description=\"Performs basic arithmetic operations\",\n",
    "    system_prompt=\"You are a calculator assistant. Use your tools for any math operation.\",\n",
    "    tools=[add, subtract],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "query_agent = ReActAgent(\n",
    "    name=\"info_lookup\",\n",
    "    description=\"Looks up information about XYZ\",\n",
    "    system_prompt=\"Use your tool to query a RAG system to answer information about XYZ\",\n",
    "    tools=[query_engine_tool],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Create and run the workflow\n",
    "agent = AgentWorkflow(\n",
    "    agents=[calculator_agent, query_agent], root_agent=\"calculator\"\n",
    ")\n",
    "\n",
    "# Run the system\n",
    "response = await agent.run(user_msg=\"Can you add 5 and 3?\")\n",
    "print (response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda10aed",
   "metadata": {},
   "source": [
    "Workflow Creation in LLamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6f631af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import StartEvent, StopEvent, Workflow, step\n",
    "\n",
    "class MyWorkflow(Workflow):\n",
    "    @step\n",
    "    async def my_step(self, ev: StartEvent) -> StopEvent:\n",
    "        # do something here\n",
    "        return StopEvent(result=\"Hello, world!\")\n",
    "\n",
    "\n",
    "w = MyWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7bbbfc1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Finished processing: Step 1 complete'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str\n",
    "\n",
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent) -> ProcessingEvent:\n",
    "        # Process initial data\n",
    "        return ProcessingEvent(intermediate_result=\"Step 1 complete\")\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
    "        # Use the intermediate result\n",
    "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
    "        return StopEvent(result=final_result)\n",
    "\n",
    "w = MultiStepWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca1bffda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good thing happened\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished processing: First step complete.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "import random\n",
    "\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str\n",
    "\n",
    "\n",
    "class LoopEvent(Event):\n",
    "    loop_output: str\n",
    "\n",
    "\n",
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent | LoopEvent) -> ProcessingEvent | LoopEvent:\n",
    "        if random.randint(0, 1) == 0:\n",
    "            print(\"Bad thing happened\")\n",
    "            return LoopEvent(loop_output=\"Back to step one.\")\n",
    "        else:\n",
    "            print(\"Good thing happened\")\n",
    "            return ProcessingEvent(intermediate_result=\"First step complete.\")\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
    "        # Use the intermediate result\n",
    "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
    "        return StopEvent(result=final_result)\n",
    "\n",
    "\n",
    "w = MultiStepWorkflow(verbose=False)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526dbe78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma_env (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
