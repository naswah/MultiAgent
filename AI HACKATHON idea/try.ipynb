{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d1fb05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U langgraph langchain-google-genai langchain-core python-dotenv google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66a58d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Hackathon Pipeline for: 'emotion detection using CNN'\n",
      "\n",
      "--- Brainstorming ---\n",
      "--- Designing ---\n",
      "--- Coding ---\n",
      "--- Testing ---\n",
      "\n",
      "Pipeline Complete!\n",
      "----------------------------------------\n",
      "Sample Output (Tech Stack from Brainstorming):\n",
      "```json\n",
      "{\n",
      "  \"title\": \"EmoNet: Real-Time Emotion Recognition\",\n",
      "  \"summary\": \"A web application that uses a Convolutional Neural Network (CNN) to detect and classify human emotions in real-time from a w...\n",
      "----------------------------------------\n",
      "\n",
      "Generating Presentation Script...\n",
      "\n",
      "Presentation Script:\n",
      "```json\n",
      "{\n",
      "  \"script\": \"(Start on Title Slide)\\n\\nHello everyone. Think about our digital conversations—on video calls, in chat—they often lack a critical piece of human connection: emotional context. We see faces, but we miss the subtle cues. How can we build technology that’s more empathetic and aware? \\n\\n(Click to next slide)\\n\\nToday, I'm introducing EmoNet, a project designed to bridge that emotional gap. EmoNet is a real-time emotion recognition application that runs directly in your browser. \\n\\n(Click to next slide)\\n\\nSo, how does it work? At its heart is a lightweight Convolutional Neural Network, or CNN, that we’ve trained on thousands of facial images. Using technologies like Python, TensorFlow, and OpenCV, our backend API processes a live webcam feed, frame by frame. It then classifies the dominant emotion—like happy, sad, angry, or neutral—and sends the result back to the user instantly.\\n\\n(Click to next slide)\\n\\nBut powerful technology means nothing if it isn't accessible. We designed the user experience to be incredibly simple and intuitive.\\n\\n(Click to next slide)\\n\\nYou start on a clean landing page with a single call to action: 'Start Camera'. There’s no complex setup.\\n\\n(Click to next slide)\\n\\nOnce clicked, you see the live detection view. Your webcam feed is front and center. A clean, non-intrusive overlay shows the detected emotion in large, clear text, along with a confidence score. The focus remains entirely on the real-time feedback. When you're done, a clear 'Stop' button puts you back in control. It's seamless and transparent.\\n\\n(Click to next slide)\\n\\nOur plan to bring this to life is broken down into five clear milestones: from preparing the dataset and training the CNN model, to building the backend API and frontend interface, and finally, integrating everything for a live demonstration.\\n\\n(Click to next slide)\\n\\nThe potential here is huge. Imagine smarter virtual assistants, feedback tools for public speaking, or even applications in mental health monitoring. EmoNet is a foundational step towards creating more emotionally intelligent technology.\\n\\n(Click to next slide)\\n\\nIt’s not just about detecting pixels; it’s about understanding people. Thank you.\",\n",
      "  \"slide_cues\": [\n",
      "    \"**Slide 1: Title Slide** - EmoNet: Real-Time Emotion Recognition. [Your Name/Team Name]\",\n",
      "    \"**Slide 2: The Problem** - A single powerful image showing a person on a video call looking disconnected. Text: 'The Emotional Gap in Digital Communication.'\",\n",
      "    \"**Slide 3: The Solution: How It Works** - A simple high-level diagram: [Webcam Icon] -> [JavaScript Frontend] -> [Flask API + CNN Model] -> [Emotion Result]. Include tech stack logos (Python, TensorFlow, OpenCV).\",\n",
      "    \"**Slide 4: The User Experience** - A title slide for the next section. Text: 'Simple, Intuitive, and Real-Time.'\",\n",
      "    \"**Slide 5: UI Wireframe: Landing Page** - Display the wireframe for the Landing Page. Title: 'Step 1: A Clear Starting Point'.\",\n",
      "    \"**Slide 6: UI Wireframe: Live Detection View** - Display the wireframe for the Live Detection View, perhaps with a sample 'HAPPY: 92%' overlay. Title: 'Step 2: Instant Feedback'.\",\n",
      "    \"**Slide 7: Project Milestones** - A clean, visual timeline or checklist graphic showing the 5 milestones from the project brainstorm (Data Preprocessing, Model Training, Backend API, Frontend UI, Integration).\",\n",
      "    \"**Slide 8: The Vision & Applications** - Use simple icons to represent potential applications: a chatbot icon, a microphone icon (public speaking), a brain/health icon (mental wellness).\",\n",
      "    \"**Slide 9: Thank You / Q&A** - A simple closing slide with the project name 'EmoNet' and contact information. Text: 'Building Emotionally Intelligent Technology.'\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import TypedDict, Optional\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    API_KEY = os.environ[\"GEMINI_PAID_KEY\"]\n",
    "except KeyError:\n",
    "    print(\"Error: The 'GEMINI_PAID_KEY' environment variable is not set in your .env file.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-pro\",  \n",
    "    google_api_key=API_KEY,\n",
    "    temperature=0.7,\n",
    "    convert_system_message_to_human=True\n",
    ")\n",
    "\n",
    "class HackathonState(TypedDict):\n",
    "    idea: str\n",
    "    brainstorm: Optional[str]\n",
    "    design: Optional[str]\n",
    "    code: Optional[str]\n",
    "    test: Optional[str]\n",
    "    presentation: Optional[str]\n",
    "\n",
    "# Define System Prompts \n",
    "PROMPTS = {\n",
    "    \"brainstorm\": \"\"\"You are a hackathon brainstorming assistant. \n",
    "    Input: {idea}.\n",
    "    Output JSON: {{ \"title\": \"...\", \"summary\": \"...\", \"features\": [], \"milestones\": [], \"tech_stack\": [] }}.\n",
    "    Be concise.\"\"\",\n",
    "    \n",
    "    \"design\": \"\"\"You are a UX designer. \n",
    "    Input Idea: {idea}\n",
    "    Brainstorm Data: {brainstorm}\n",
    "    Output JSON: {{ \"screens\": [], \"wireframes\": [], \"ui_notes\": \"...\" }}. \n",
    "    Be concise.\"\"\",\n",
    "    \n",
    "    \"code\": \"\"\"You are a developer coach.\n",
    "    Input Idea: {idea}\n",
    "    Design Specs: {design}\n",
    "    Output JSON: {{ \"snippets\": [{{ \"name\": \"...\", \"language\": \"...\", \"code\": \"...\", \"explanation\": \"...\" }}], \"setup_steps\": [] }}.\"\"\",\n",
    "    \n",
    "    \"test\": \"\"\"You are a QA advisor.\n",
    "    Input Code Plan: {code}\n",
    "    Output JSON: {{ \"checklist\": [], \"example_unit_tests\": [] }}.\"\"\",\n",
    "    \n",
    "    \"presentation\": \"\"\"You are a presentation coach.\n",
    "    Full Project Context: \n",
    "    - Idea: {idea}\n",
    "    - Brainstorm: {brainstorm}\n",
    "    - Design: {design}\n",
    "    Output: A 3-minute presentation script and slide cues in JSON: {{ \"script\": \"...\", \"slide_cues\": [] }}.\"\"\"\n",
    "}\n",
    "\n",
    "# Each function receives the current state, runs the LLM, and returns the *diff* to update the state.\n",
    "\n",
    "def brainstorm_step(state: HackathonState):\n",
    "    print(\"--- Brainstorming ---\")\n",
    "    msg = PROMPTS[\"brainstorm\"].format(idea=state[\"idea\"])\n",
    "    response = llm.invoke(msg)\n",
    "    return {\"brainstorm\": response.content}\n",
    "\n",
    "def design_step(state: HackathonState):\n",
    "    print(\"--- Designing ---\")\n",
    "    # We can access previous outputs (brainstorm) from the state\n",
    "    msg = PROMPTS[\"design\"].format(idea=state[\"idea\"], brainstorm=state[\"brainstorm\"])\n",
    "    response = llm.invoke(msg)\n",
    "    return {\"design\": response.content}\n",
    "\n",
    "def code_step(state: HackathonState):\n",
    "    print(\"--- Coding ---\")\n",
    "    msg = PROMPTS[\"code\"].format(idea=state[\"idea\"], design=state[\"design\"])\n",
    "    response = llm.invoke(msg)\n",
    "    return {\"code\": response.content}\n",
    "\n",
    "def test_step(state: HackathonState):\n",
    "    print(\"--- Testing ---\")\n",
    "    msg = PROMPTS[\"test\"].format(code=state[\"code\"])\n",
    "    response = llm.invoke(msg)\n",
    "    return {\"test\": response.content}\n",
    "\n",
    "# Graph creation\n",
    "workflow = StateGraph(HackathonState)\n",
    "\n",
    "# Add Nodes\n",
    "workflow.add_node(\"brainstorm\", brainstorm_step)\n",
    "workflow.add_node(\"design\", design_step)\n",
    "workflow.add_node(\"code\", code_step)\n",
    "workflow.add_node(\"test\", test_step)\n",
    "\n",
    "# Define Edges \n",
    "workflow.set_entry_point(\"brainstorm\")\n",
    "workflow.add_edge(\"brainstorm\", \"design\")\n",
    "workflow.add_edge(\"design\", \"code\")\n",
    "workflow.add_edge(\"code\", \"test\")\n",
    "workflow.add_edge(\"test\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "#  On-Demand Presentation if the user click y ---\n",
    "def generate_presentation(final_state: HackathonState):\n",
    "    \"\"\"\n",
    "    This runs separately from the main graph, using the final accumulated state.\n",
    "    \"\"\"\n",
    "    print(\"\\nGenerating Presentation Script...\")\n",
    "    msg = PROMPTS[\"presentation\"].format(\n",
    "        idea=final_state[\"idea\"],\n",
    "        brainstorm=final_state[\"brainstorm\"],\n",
    "        design=final_state[\"design\"]\n",
    "    )\n",
    "    response = llm.invoke(msg)\n",
    "    return response.content\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_idea = \"emotion detection using CNN\"\n",
    "    \n",
    "    print(f\"Starting Hackathon Pipeline for: '{user_idea}'\\n\")\n",
    "    initial_state = {\"idea\": user_idea}\n",
    "    \n",
    "    # Run the pipeline (Brainstorm -> Design -> Code -> Test)\n",
    "    final_state = app.invoke(initial_state)\n",
    "    \n",
    "    print(\"\\nPipeline Complete!\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Sample Output (Tech Stack from Brainstorming):\")\n",
    "    # Just printing a snippet to verify it worked\n",
    "    print(final_state[\"brainstorm\"][:200] + \"...\") \n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    user_input = input(\"\\nWould you like to generate the presentation script? (y/n): \")\n",
    "    if user_input.lower().startswith('y'):\n",
    "        script = generate_presentation(final_state)\n",
    "        print(\"\\nPresentation Script:\")\n",
    "        print(script)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma_env (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
