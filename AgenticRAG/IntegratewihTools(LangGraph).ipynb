{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa59c1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current president is Emmanuel Macron , who succeeded François Hollande on 14 May 2017 , and was inaugurated for a second term on 7 May 2022. The current president is Emmanuel Macron , who succeeded François Hollande on 14 May 2017 following the 2017 presidential election , and was ... Emmanuel Macron, France ’ s former economy minister, who led the first round of presidential elections on April 23, will very likely be the Fifth ... François Gérard Georges Hollande (born 12 August 1954) is a French politician who is the President -elect of France . Emmanual Macron, the current President of France . ... sworn to block employment of cheap labor from eastern Europe in an attempt to secure the France ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "results = search_tool.invoke(\"Who's the current President of France?\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7940698f",
   "metadata": {},
   "source": [
    "making tool example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9c7caf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather in Kathmandu :WINDY, 20 C\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import Tool\n",
    "import random\n",
    "\n",
    "def get_weather(location: str)-> str:\n",
    "    weather_conditions=[\n",
    "        {\"condition\": \"RAINY\", \"temp_c\":15},\n",
    "        {\"condition\": \"CLEAR\", \"temp_c\":25},\n",
    "        {\"condition\": \"WINDY\", \"temp_c\":20},\n",
    "    ]\n",
    "    data= random.choice(weather_conditions)\n",
    "    return f\"Weather in {location} :{data['condition']}, {data['temp_c']} C\"\n",
    "\n",
    "weather_info_tool= Tool(\n",
    "    name=\"get_weateher\",\n",
    "    func= get_weather,\n",
    "    description=\"Fetches dummy weather information for a given location.\"\n",
    ")                    \n",
    "print(weather_info_tool.invoke(\"Kathmandu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b8708a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most downloaded model by facebook is facebook/contriever with 7,617,234 downloads\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import list_models\n",
    "\n",
    "def get_hub_stats(author: str) -> str:\n",
    "    try:\n",
    "        models = list(list_models(author=author, sort=\"downloads\", direction=-1, limit=1))\n",
    "\n",
    "        if models:\n",
    "            model = models[0]\n",
    "            model_id = getattr(model, \"modelId\", getattr(model, \"id\", \"Unknown\"))\n",
    "            downloads = getattr(model, \"downloads\", \"Unknown\")\n",
    "            return f\"The most downloaded model by {author} is {model_id} with {downloads:,} downloads\"\n",
    "        else:\n",
    "            return f\"No models found for author {author}.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching models for {author}: {str(e)}\"\n",
    "\n",
    "hub_stats_tool = Tool(\n",
    "    name=\"get_hub_stats\",\n",
    "    func=get_hub_stats,\n",
    "    description=\"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\"\n",
    ")\n",
    "\n",
    "print(hub_stats_tool.invoke(\"facebook\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee51220f",
   "metadata": {},
   "source": [
    "Integrating the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c6d138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    API_KEY = os.environ[\"GEMINI_PAID_KEY\"]\n",
    "except KeyError:\n",
    "    print(\"Error: The 'gemini_api_key' environment variable is not set.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-2.5-pro\",        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9678b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from langchain_community.tools import DuckDuckGoSearchRun\n",
    "# from typing import TypedDict, List\n",
    "# from langgraph.graph import StateGraph, START\n",
    "# from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n",
    "# from dotenv import load_dotenv\n",
    "# import google.generativeai as genai\n",
    "# from google.genai import types\n",
    "\n",
    "# load_dotenv()\n",
    "# try:\n",
    "#     GEMINI_API_KEY = os.environ[\"gemini_api_key\"]\n",
    "# except KeyError:\n",
    "#     print(\"Error: The 'gemini_api_key' environment variable is not set.\")\n",
    "#     exit(1)\n",
    "\n",
    "# genai.configure(api_key=GEMINI_API_KEY)\n",
    "# model = genai.GenerativeModel(model_name=\"gemini-2.5-pro\")\n",
    "\n",
    "# class AgentState(TypedDict):\n",
    "#     messages: List[AnyMessage]\n",
    "\n",
    "# MAX_HISTORY = 3\n",
    "\n",
    "# def call_gemini(prompt: str, state: AgentState) -> AgentState:\n",
    "#     last_messages = state[\"messages\"][-MAX_HISTORY:]\n",
    "#     contents = []\n",
    "\n",
    "#     for msg in last_messages:\n",
    "#         role = \"assistant\" if isinstance(msg, AIMessage) else \"user\"\n",
    "#         text = msg.content[:500]\n",
    "#         contents.append(\n",
    "#             types.Content(\n",
    "#                 role=role,\n",
    "#                 parts=[types.Part(text=text)]\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#     contents.append(\n",
    "#         types.Content(\n",
    "#             role=\"user\",\n",
    "#             # Ensure we use 'prompt' here\n",
    "#             parts=[types.Part(text=prompt[:1000])]\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     response = model.generate_content(\n",
    "#         contents=contents,\n",
    "#         generation_config=types.GenerateContentConfig(\n",
    "#             max_output_tokens=500\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     ai_text = response.text.strip()\n",
    "#     ai_msg = AIMessage(content=ai_text)\n",
    "#     return {\"messages\": [ai_msg]}\n",
    "\n",
    "# def assistant(state: AgentState) -> AgentState:\n",
    "#     messages_content = []\n",
    "#     for msg in state[\"messages\"][-MAX_HISTORY:]:\n",
    "#         try:\n",
    "#             content = msg.content\n",
    "#             if content is not None:\n",
    "#                 messages_content.append(content)\n",
    "#         except AttributeError:\n",
    "#             continue\n",
    "            \n",
    "#     last_messages = \"\\n\".join(messages_content)\n",
    "    \n",
    "#     prompt = f\"As an assistant, respond to the following user query. The conversation history is:\\n\\n{last_messages}\"\n",
    "#     return call_gemini(prompt, state)\n",
    "\n",
    "# tools = {\n",
    "#     \"search_tool\": search_tool,\n",
    "#     \"weather_tool\": weather_info_tool,\n",
    "#     \"hub_stats_tool\": hub_stats_tool\n",
    "# }\n",
    "\n",
    "# def route_tool(state: AgentState) -> str:\n",
    "#     last_msg = state[\"messages\"][-1].content.lower()\n",
    "#     if \"facebook\" in last_msg or \"popular model\" in last_msg:\n",
    "#         return \"hub_stats_tool\"\n",
    "#     elif \"weather\" in last_msg:\n",
    "#         return \"weather_tool\"\n",
    "#     elif \"who\" in last_msg or \"search\" in last_msg:\n",
    "#         return \"search_tool\"\n",
    "#     else:\n",
    "#         return \"assistant\"\n",
    "\n",
    "# builder = StateGraph(AgentState)\n",
    "# builder.add_node(\"assistant\", assistant)\n",
    "\n",
    "# for tool_name, tool_func in tools.items():\n",
    "#     builder.add_node(tool_name, tool_func)\n",
    "\n",
    "# builder.add_edge(START, \"assistant\")\n",
    "\n",
    "# builder.add_conditional_edges(\"assistant\", route_tool, {\n",
    "#     \"search_tool\": \"search_tool\",\n",
    "#     \"weather_tool\": \"weather_tool\",\n",
    "#     \"hub_stats_tool\": \"hub_stats_tool\",\n",
    "#     \"assistant\": \"assistant\"\n",
    "# })\n",
    "\n",
    "# for tool_name in tools.keys():\n",
    "#     builder.add_edge(tool_name, \"assistant\")\n",
    "\n",
    "# alfred = builder.compile()\n",
    "\n",
    "# messages = [HumanMessage(content=\"Who is Facebook and what's their most popular model?\")]\n",
    "# response = alfred.invoke({\"messages\": messages})\n",
    "\n",
    "# print(\"Alfred's Response:\")\n",
    "# print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74ec03eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDefaultCredentialsError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     18\u001b[39m     exit(\u001b[32m1\u001b[39m)\n\u001b[32m     20\u001b[39m genai.configure(api_key=GEMINI_API_KEY)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m llm = \u001b[43mChatGoogleGenerativeAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini-2.5-pro\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\n\u001b[32m     25\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mAgentState\u001b[39;00m(TypedDict):\n\u001b[32m     29\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Represents the state of the agent in the graph.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Multi Agent/ma_env/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1951\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   1944\u001b[39m         suggestion = (\n\u001b[32m   1945\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Did you mean: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestions[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m?\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suggestions \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1946\u001b[39m         )\n\u001b[32m   1947\u001b[39m         logger.warning(\n\u001b[32m   1948\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected argument \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1949\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mprovided to ChatGoogleGenerativeAI.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1950\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1951\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Multi Agent/ma_env/lib/python3.13/site-packages/langchain_core/load/serializable.py:116\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Multi Agent/ma_env/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:2028\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.validate_environment\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.base_url \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mapi_endpoint\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m client_options:\n\u001b[32m   2026\u001b[39m     client_options = {**client_options, \u001b[33m\"\u001b[39m\u001b[33mapi_endpoint\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.base_url}\n\u001b[32m-> \u001b[39m\u001b[32m2028\u001b[39m \u001b[38;5;28mself\u001b[39m.client = \u001b[43mgenaix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_generative_service\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2029\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgoogle_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2031\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2032\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2033\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2034\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2035\u001b[39m \u001b[38;5;28mself\u001b[39m.async_client_running = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2036\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Multi Agent/ma_env/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:360\u001b[39m, in \u001b[36mbuild_generative_service\u001b[39m\u001b[34m(credentials, api_key, client_options, client_info, transport)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_generative_service\u001b[39m(\n\u001b[32m    347\u001b[39m     credentials: credentials.Credentials | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    348\u001b[39m     api_key: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    351\u001b[39m     transport: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    352\u001b[39m ) -> v1betaGenerativeServiceClient:\n\u001b[32m    353\u001b[39m     config = _prepare_config(\n\u001b[32m    354\u001b[39m         credentials=credentials,\n\u001b[32m    355\u001b[39m         api_key=api_key,\n\u001b[32m   (...)\u001b[39m\u001b[32m    358\u001b[39m         client_info=client_info,\n\u001b[32m    359\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mv1betaGenerativeServiceClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Multi Agent/ma_env/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:698\u001b[39m, in \u001b[36mGenerativeServiceClient.__init__\u001b[39m\u001b[34m(self, credentials, transport, client_options, client_info)\u001b[39m\n\u001b[32m    689\u001b[39m     transport_init: Union[\n\u001b[32m    690\u001b[39m         Type[GenerativeServiceTransport],\n\u001b[32m    691\u001b[39m         Callable[..., GenerativeServiceTransport],\n\u001b[32m   (...)\u001b[39m\u001b[32m    695\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m cast(Callable[..., GenerativeServiceTransport], transport)\n\u001b[32m    696\u001b[39m     )\n\u001b[32m    697\u001b[39m     \u001b[38;5;66;03m# initialize with the provided callable or the passed in class\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m698\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport = \u001b[43mtransport_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcredentials_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_endpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient_cert_source_for_mtls\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client_cert_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m        \u001b[49m\u001b[43malways_use_jwt_access\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_audience\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapi_audience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33masync\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m._transport):\n\u001b[32m    711\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m CLIENT_LOGGING_SUPPORTED \u001b[38;5;129;01mand\u001b[39;00m _LOGGER.isEnabledFor(\n\u001b[32m    712\u001b[39m         std_logging.DEBUG\n\u001b[32m    713\u001b[39m     ):  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Multi Agent/ma_env/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:235\u001b[39m, in \u001b[36mGenerativeServiceGrpcTransport.__init__\u001b[39m\u001b[34m(self, host, credentials, credentials_file, scopes, channel, api_mtls_endpoint, client_cert_source, ssl_channel_credentials, client_cert_source_for_mtls, quota_project_id, client_info, always_use_jwt_access, api_audience)\u001b[39m\n\u001b[32m    230\u001b[39m             \u001b[38;5;28mself\u001b[39m._ssl_channel_credentials = grpc.ssl_channel_credentials(\n\u001b[32m    231\u001b[39m                 certificate_chain=cert, private_key=key\n\u001b[32m    232\u001b[39m             )\n\u001b[32m    234\u001b[39m \u001b[38;5;66;03m# The base transport sets the host, credentials and scopes\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43malways_use_jwt_access\u001b[49m\u001b[43m=\u001b[49m\u001b[43malways_use_jwt_access\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_audience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_audience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._grpc_channel:\n\u001b[32m    247\u001b[39m     \u001b[38;5;66;03m# initialize with the provided callable or the default channel\u001b[39;00m\n\u001b[32m    248\u001b[39m     channel_init = channel \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).create_channel\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Multi Agent/ma_env/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:105\u001b[39m, in \u001b[36mGenerativeServiceTransport.__init__\u001b[39m\u001b[34m(self, host, credentials, credentials_file, scopes, quota_project_id, client_info, always_use_jwt_access, api_audience, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m     credentials, _ = google.auth.load_credentials_from_file(\n\u001b[32m    102\u001b[39m         credentials_file, **scopes_kwargs, quota_project_id=quota_project_id\n\u001b[32m    103\u001b[39m     )\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m credentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._ignore_credentials:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     credentials, _ = \u001b[43mgoogle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mscopes_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquota_project_id\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;66;03m# Don't apply audience if the credentials file passed from user.\u001b[39;00m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(credentials, \u001b[33m\"\u001b[39m\u001b[33mwith_gdch_audience\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Multi Agent/ma_env/lib/python3.13/site-packages/google/auth/_default.py:739\u001b[39m, in \u001b[36mdefault\u001b[39m\u001b[34m(scopes, request, quota_project_id, default_scopes)\u001b[39m\n\u001b[32m    731\u001b[39m             _LOGGER.warning(\n\u001b[32m    732\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mNo project ID could be determined. Consider running \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    733\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m`gcloud config set project` or setting the \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    734\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33menvironment variable\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    735\u001b[39m                 environment_vars.PROJECT,\n\u001b[32m    736\u001b[39m             )\n\u001b[32m    737\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m credentials, effective_project_id\n\u001b[32m--> \u001b[39m\u001b[32m739\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "\u001b[31mDefaultCredentialsError\u001b[39m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import TypedDict, List\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from huggingface_hub import list_models\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    GEMINI_API_KEY = os.environ[\"gemini_api_key\"]\n",
    "except KeyError:\n",
    "    print(\"Error: The 'gemini_api_key' environment variable is not set.\")\n",
    "    exit(1)\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-pro\", \n",
    "        max_output_tokens=500\n",
    ")\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"Represents the state of the agent in the graph.\"\"\"\n",
    "    messages: List[AnyMessage]\n",
    "\n",
    "tools_map = {\n",
    "    \"search_tool\": search_tool,\n",
    "    \"weather_tool\": weather_info_tool,\n",
    "    \"hub_stats_tool\": hub_stats_tool\n",
    "}\n",
    "\n",
    "def call_llm(state: AgentState) -> AgentState:\n",
    "    \"\"\"The assistant node, which calls the LLM with the message history.\"\"\"\n",
    "    \n",
    "    messages_to_send = [SystemMessage(content=\"You are a helpful multi-agent assistant.\")] + state[\"messages\"]\n",
    "\n",
    "    response_message = llm.invoke(messages_to_send)\n",
    "    \n",
    "    return {\"messages\": [response_message]}\n",
    "\n",
    "assistant = call_llm\n",
    "\n",
    "\n",
    "def tool_node_factory(tool_name: str, tool_func: Tool):\n",
    "    \"\"\"Generates a wrapper node function for a given tool.\"\"\"\n",
    "    def wrapper(state: AgentState) -> AgentState:\n",
    "        query = state[\"messages\"][-1].content\n",
    "        \n",
    "        # Invoke the tool\n",
    "        tool_output = tool_func.invoke(query)\n",
    "        \n",
    "        # Create an AIMessage containing the tool's result\n",
    "        ai_msg = AIMessage(content=f\"Tool ({tool_name}) Result: {tool_output}\")\n",
    "        \n",
    "        # Return ONLY the new message for LangGraph to append\n",
    "        return {\"messages\": [ai_msg]}\n",
    "    return wrapper\n",
    "\n",
    "search_tool_node = tool_node_factory(\"search_tool\", search_tool)\n",
    "weather_tool_node = tool_node_factory(\"weather_tool\", weather_info_tool)\n",
    "hub_stats_tool_node = tool_node_factory(\"hub_stats_tool\", hub_stats_tool)\n",
    "\n",
    "\n",
    "def route_tool(state: AgentState) -> str:\n",
    "    \"\"\"Decides which tool to call next based on the last message.\"\"\"\n",
    "    last_msg = state[\"messages\"][-1].content.lower()\n",
    "    \n",
    "    if \"facebook\" in last_msg or \"popular model\" in last_msg or \"meta\" in last_msg:\n",
    "        return \"hub_stats_tool\"\n",
    "    elif \"weather\" in last_msg or \"forecast\" in last_msg:\n",
    "        return \"weather_tool\"\n",
    "    elif \"who is\" in last_msg or \"search\" in last_msg:\n",
    "        return \"search_tool\"\n",
    "    else:\n",
    "        return \"assistant\"\n",
    "\n",
    "\n",
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "\n",
    "tool_nodes = {\n",
    "    \"search_tool\": search_tool_node,\n",
    "    \"weather_tool\": weather_tool_node,\n",
    "    \"hub_stats_tool\": hub_stats_tool_node\n",
    "}\n",
    "for tool_name, tool_func in tool_nodes.items():\n",
    "    builder.add_node(tool_name, tool_func)\n",
    "\n",
    "builder.add_edge(START, \"assistant\")\n",
    "\n",
    "builder.add_conditional_edges(\"assistant\", route_tool, {\n",
    "    \"search_tool\": \"search_tool\",\n",
    "    \"weather_tool\": \"weather_tool\",\n",
    "    \"hub_stats_tool\": \"hub_stats_tool\",\n",
    "    \"assistant\": \"assistant\"\n",
    "})\n",
    "\n",
    "for tool_name in tool_nodes.keys():\n",
    "    builder.add_edge(tool_name, \"assistant\")\n",
    "\n",
    "alfred = builder.compile()\n",
    "\n",
    "messages = [HumanMessage(content=\"Who is Facebook and what's their most popular model?\")]\n",
    "print(\"Running Alfred Agent...\")\n",
    "\n",
    "response = alfred.invoke({\"messages\": messages})\n",
    "\n",
    "print(\"\\n--- Alfred's Final Response ---\")\n",
    "print(response[\"messages\"][-1].content)\n",
    "print(\"-------------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma_env (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
